{
  "config": {
    "step": {
      "user": {
        "title": "Connect models",
        "description": "Enter **API keys**, optional **Ollama server URLs** (global or per-model), **Postgres URI** or **Face recognition server URI**. You must enter a valid **Postgres URI**, the others you can configure later in Options.",
        "data": {
          "api_key": "OpenAI API key (optional)",
          "ollama_url": "Ollama server URL (optional)",
          "ollama_chat_url": "Ollama chat server URL (optional)",
          "ollama_vlm_url": "Ollama VLM server URL (optional)",
          "ollama_summarization_url": "Ollama summarization server URL (optional)",
          "gemini_api_key": "Google Gemini API key (optional)",
          "face_api_url": "Face recognition server API URL (optional)",
          "db_uri": "PostgreSQL Database URI (required)"
        }
      }
    },
    "error": {
      "cannot_connect": "Can't reach the service. Check keys/URLs and try again.",
      "invalid_auth": "Invalid API key.",
      "invalid_pin": "PIN must be 4-10 digits.",
      "unknown": "Unexpected error."
    }
  },
  "exceptions": {
    "invalid_config_entry": {
      "message": "Invalid config entry provided. Got {config_entry}"
    }
  },
  "options": {
    "step": {
      "init": {
        "title": "Home Generative Agent Options",
        "description": "Configure Keys, URLs, main prompt, models and providers.",
        "data": {
          "api_key": "OpenAI API key (optional)",
          "gemini_api_key": "Google Gemini API key (optional)",
          "ollama_url": "Ollama embedding and global fallback server URL (optional)",
          "ollama_chat_url": "Ollama chat server URL (optional)",
          "ollama_vlm_url": "Ollama VLM server URL (optional)",
          "ollama_summarization_url": "Ollama summarization server URL (optional)",
          "face_api_url": "Face recognition API URL (optional)",
          "db_uri": "PostgreSQL Database URI (required)",
          "prompt": "Instructions",
          "chat_model_provider": "Chat model provider",
          "ollama_chat_model": "Ollama chat model",
          "openai_chat_model": "OpenAI chat model",
          "gemini_chat_model": "Gemini chat model",
          "chat_model_temperature": "Chat model temperature",
          "vlm_provider": "VLM provider",
          "ollama_vlm": "Ollama VLM",
          "openai_vlm": "OpenAI VLM",
          "gemini_vlm": "Gemini VLM",
          "vlm_temperature": "VLM temperature",
          "summarization_provider": "Summarization provider",
          "ollama_summarization_model": "Ollama summarization model",
          "openai_summarization_model": "OpenAI summarization model",
          "gemini_summarization_model": "Gemini summarization model",
          "summarization_model_temperature": "Summarization model temperature",
          "embedding_model_provider": "Embedding model provider",
          "ollama_embedding_model": "Ollama embedding model",
          "openai_embedding_model": "OpenAI embedding model",
          "gemini_embedding_model": "Gemini embedding model",
          "llm_hass_api": "Control Home Assistant",
          "recommended": "Recommended model settings (changing these may impact performance)",
          "video_analyzer_mode": "Video analyzer mode",
          "notify_service": "Mobile App Notify Service (optional)",
          "face_recognition": "Enable face recognition",
          "ollama_reasoning": "Enable Ollama reasoning",
          "ollama_chat_keepalive": "Ollama chat model keepalive (seconds or Never unload (-1))",
          "ollama_vlm_keepalive": "Ollama VLM keepalive (seconds or Never unload (-1))",
          "ollama_summarization_keepalive": "Ollama summarization model keepalive (seconds or Never unload (-1))",
          "ollama_chat_context_size": "Ollama chat model context size (tokens)",
          "ollama_vlm_context_size": "Ollama VLM context size (tokens)",
          "ollama_summarization_context_size": "Ollama summarization model context size (tokens)",
          "manage_context_with_tokens": "Context management method",
          "max_tokens_in_context": "Max tokens in chat context",
          "max_messages_in_context": "Max messages in chat context",
          "critical_action_pin_enabled": "Require critical action PIN",
          "critical_action_pin": "Critical action PIN (required to unlock/arm/open sensitive devices)"
        },
        "data_description": {
          "prompt": "Instruct how the LLM should respond. This can be a template."
        }
      }
    }
  }
}